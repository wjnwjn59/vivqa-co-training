{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202fdec9",
   "metadata": {},
   "source": [
    "Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def find_high_similarity_questions(json_data, threshold=0.9):\n",
    "    result = {}\n",
    "    for image_id, image_data in json_data.items():\n",
    "        for category in ['generated_vs_original', 'generated_vs_paraphrased']:\n",
    "            if category in image_data:\n",
    "                for gen_q_key, questions in image_data[category].items():\n",
    "                    for q_key, details in questions.items():\n",
    "                        if details['similarity_score'] > threshold:\n",
    "                            if image_id not in result:\n",
    "                                result[image_id] = []\n",
    "                            if gen_q_key not in result[image_id]:\n",
    "                                result[image_id].append(gen_q_key)\n",
    "    return result\n",
    "\n",
    "def remove_duplicates(origin_json, duplicated_json):\n",
    "    filtered_json = {}\n",
    "    for image_id, content in origin_json.items():\n",
    "        filtered_content = json.loads(json.dumps(content))\n",
    "        if \"qwenvl_generated\" in filtered_content and \"question_generated\" in filtered_content[\"qwenvl_generated\"]:\n",
    "            generated_questions = filtered_content[\"qwenvl_generated\"][\"question_generated\"]\n",
    "            if image_id in duplicated_json:\n",
    "                for gen_q_key in duplicated_json[image_id]:\n",
    "                    if gen_q_key in generated_questions:\n",
    "                        del generated_questions[gen_q_key]\n",
    "        filtered_json[image_id] = filtered_content\n",
    "    return filtered_json\n",
    "\n",
    "def main():\n",
    "    with open('data/duplicated/score_train.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    high_similarity = find_high_similarity_questions(data)\n",
    "    \n",
    "    # Write duplicated json and ensure file is closed before reading\n",
    "    with open('data/duplicated/duplicated_train.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(high_similarity, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    # Now read the duplicated json after the write is complete\n",
    "    with open('data/duplicated/duplicated_train.json', 'r', encoding='utf-8') as f:\n",
    "        duplicated_json = json.load(f)\n",
    "        \n",
    "    with open('data/qwenvl_openvivqa/qwenvl_train.json', 'r', encoding='utf-8') as f:\n",
    "        origin_json = json.load(f)\n",
    "        \n",
    "    filtered_json = remove_duplicates(origin_json, duplicated_json)\n",
    "    \n",
    "    with open('data/qwenvl_openvivqa/qwenvl_train_filtered.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_json, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    return high_similarity, filtered_json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ba188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "def extract_questions_by_image_id(data):\n",
    "    \"\"\"\n",
    "    Extract and organize questions from a dataset by their image_id.\n",
    "    \n",
    "    This function takes a JSON structure containing annotations with image_id and question fields,\n",
    "    groups all questions that belong to the same image, and restructures them into a new format\n",
    "    with up to three questions per image (filling empty slots with empty strings).\n",
    "    \n",
    "    Args:\n",
    "        data (dict): The input data dictionary containing an 'annotations' key with question data\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary where each key is an image_id (as string) with structured question data\n",
    "              Format: {\n",
    "                  \"image_id\": The image ID (integer),\n",
    "                  \"original_question\": {\n",
    "                      \"question_1\": \"First question for this image\",\n",
    "                      \"question_2\": \"Second question for this image (if exists)\",\n",
    "                      \"question_3\": \"Third question for this image (if exists)\"\n",
    "                  }\n",
    "              }\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    # Temporary dictionary to hold questions grouped by image_id\n",
    "    temp = {}\n",
    "\n",
    "    # Step 1: Group all questions by their respective image_id\n",
    "    for key, value in data[\"annotations\"].items():\n",
    "        image_id = value[\"image_id\"]\n",
    "        question = value[\"question\"]\n",
    "\n",
    "        # Create a new list for this image_id if we haven't seen it before\n",
    "        if image_id not in temp:\n",
    "            temp[image_id] = []\n",
    "        # Add the question to the list for this image_id\n",
    "        temp[image_id].append(question)\n",
    "\n",
    "    # Step 2: Format the grouped questions into the required output structure\n",
    "    for image_id, questions in temp.items():\n",
    "        question_dict = {}\n",
    "        # Assign each question to a numbered key (question_1, question_2, etc.)\n",
    "        for i, q in enumerate(questions, 1):\n",
    "            question_dict[f\"question_{i}\"] = q\n",
    "        \n",
    "        # Add the formatted entry to the result dictionary using image_id as key\n",
    "        result[str(image_id)] = {\n",
    "            \"image_id\": image_id,\n",
    "            \"original_question\": question_dict\n",
    "        }\n",
    "\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    # Setup argument parser\n",
    "    parser = argparse.ArgumentParser(description='Process input and output JSON files for question extraction')\n",
    "    parser.add_argument('--input_json', type=str, required=True, \n",
    "                        help='Path to the input JSON file with annotations')\n",
    "    parser.add_argument('--output_json', type=str, required=True, \n",
    "                        help='Path to the output JSON file for processed questions')\n",
    "\n",
    "    # Parse arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Step 1: Load the input JSON file containing question annotations\n",
    "    try:\n",
    "        with open(args.input_json, 'r', encoding='utf-8') as f:\n",
    "            input_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file {args.input_json} not found. Please check the file path.\")\n",
    "        exit(1)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in the input file {args.input_json}.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Step 2: Process the data to extract and structure questions by image_id\n",
    "    output_json = extract_questions_by_image_id(input_data)\n",
    "\n",
    "    # Step 3: Save the processed data to a new JSON file\n",
    "    try:\n",
    "        with open(args.output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_json, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Successfully processed and saved data for {len(output_json)} images.\")\n",
    "    except IOError:\n",
    "        print(f\"Error: Unable to write to output file {args.output_json}. Please check permissions and path.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0171658",
   "metadata": {},
   "source": [
    "Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b618b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "\n",
    "def remove_unqualified_questions(unqualified_questions, origin_data, target_image_id):\n",
    "    # Extract unqualified question texts for the target image_id\n",
    "    unqualified_texts = set()\n",
    "    for item in unqualified_questions:\n",
    "        if item.get('ID') == target_image_id:\n",
    "            for key, value in item.items():\n",
    "                if key.startswith('generated_question_'):\n",
    "                    unqualified_texts.add(value)\n",
    "\n",
    "    # Filter origin data for the target image_id\n",
    "    if str(target_image_id) not in origin_data:\n",
    "        return origin_data  # No changes if image_id not found\n",
    "\n",
    "    data = origin_data[str(target_image_id)]\n",
    "    if 'qwenvl_generated' not in data or 'question_generated' not in data['qwenvl_generated']:\n",
    "        return origin_data  # No generated questions to filter\n",
    "\n",
    "    generated_questions = data['qwenvl_generated']['question_generated']\n",
    "\n",
    "    # Remove unqualified generated questions\n",
    "    filtered_questions = {k: v for k, v in generated_questions.items() if v not in unqualified_texts}\n",
    "\n",
    "    # Update the origin data with filtered questions\n",
    "    origin_data[str(target_image_id)]['qwenvl_generated']['question_generated'] = filtered_questions\n",
    "\n",
    "    return origin_data\n",
    "\n",
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser(description='Qwen2.5-VL Question Evaluation')\n",
    "    \n",
    "    parser.add_argument('--config', type=str, help='Path to YAML config file')\n",
    "    parser.add_argument('--input_json', type=str, help='Path to input JSON file with questions')\n",
    "    parser.add_argument('--unqualified_json', type=str, help='Path to JSON file with unqualified questions')\n",
    "    parser.add_argument('--output_dir', type=str, help='Directory to save output JSON files')\n",
    "\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Process config and arguments\n",
    "    config = {}\n",
    "    if args.config:\n",
    "        with open(args.config, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    \n",
    "    args_dict = vars(args)\n",
    "    merged_config = {}\n",
    "    for key in args_dict:\n",
    "        if key == 'config': continue\n",
    "        if args_dict[key] is not None:\n",
    "            merged_config[key] = args_dict[key]\n",
    "        elif key in config:\n",
    "            merged_config[key] = config[key]\n",
    "    \n",
    "    # Validate required arguments\n",
    "    required_args = ['input_json', 'unqualified_json', 'output_dir']\n",
    "    missing_args = [arg for arg in required_args if arg not in merged_config]\n",
    "    if missing_args:\n",
    "        print(f\"Missing required arguments: {', '.join(missing_args)}\")\n",
    "        parser.print_help()\n",
    "        return\n",
    "\n",
    "    # Load input JSON\n",
    "    with open(merged_config['input_json'], 'r', encoding='utf-8') as f:\n",
    "        origin_data = json.load(f)\n",
    "\n",
    "    # Load unqualified questions JSON\n",
    "    with open(merged_config['unqualified_json'], 'r', encoding='utf-8') as f:\n",
    "        unqualified_questions = json.load(f)\n",
    "\n",
    "    # Remove unqualified questions for each image_id in origin_data\n",
    "    for image_id in origin_data.keys():\n",
    "        origin_data = remove_unqualified_questions(unqualified_questions, origin_data, int(image_id))\n",
    "\n",
    "    # Save filtered data to output directory\n",
    "    output_path = merged_config['output_dir']\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(origin_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Filtered questions saved to {output_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ac3bdf",
   "metadata": {},
   "source": [
    "merge_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c3947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "\n",
    "def merge_json(orig_path, gen_path, paraphr_path, output_path):\n",
    "    \"\"\"\n",
    "    Merges original Q&A, generated questions, and paraphrased questions\n",
    "    based on image_id into one JSON file with dynamic slot counts.\n",
    "\n",
    "    Args:\n",
    "        orig_path (str): Path to the original Q&A JSON file\n",
    "        gen_path (str): Path to the generated questions JSON file\n",
    "        paraphr_path (str): Path to the paraphrased questions JSON file\n",
    "        output_path (str): Path where to save the merged output JSON\n",
    "    \"\"\"\n",
    "\n",
    "    # Load source files\n",
    "    with open(orig_path, 'r', encoding='utf-8') as f:\n",
    "        orig_data = json.load(f)\n",
    "    with open(gen_path, 'r', encoding='utf-8') as f:\n",
    "        gen_data = json.load(f)\n",
    "    with open(paraphr_path, 'r', encoding='utf-8') as f:\n",
    "        paraphr_data = json.load(f)\n",
    "\n",
    "    # Group original questions & answers by image_id\n",
    "    qa_map = {}\n",
    "    for ann in orig_data.get(\"annotations\", {}).values():\n",
    "        img_id = ann.get(\"image_id\")\n",
    "        if img_id is None:\n",
    "            continue\n",
    "        qa_map.setdefault(img_id, {\"questions\": [], \"answers\": []})\n",
    "        qa_map[img_id][\"questions\"].append(ann.get(\"question\", \"\"))\n",
    "        qa_map[img_id][\"answers\"].append(ann.get(\"answer\", \"\"))\n",
    "\n",
    "    # Group generated (alternate) questions by image_id\n",
    "    gen_map = {}\n",
    "    for item in gen_data.values():\n",
    "        img_id = item.get(\"image_id\")\n",
    "        if img_id is None:\n",
    "            continue\n",
    "        alt = item.get(\"question_generated\", {})\n",
    "        ordered = [\n",
    "            alt[k] for k in sorted(\n",
    "                alt.keys(), key=lambda x: int(x.rsplit(\"_\", 1)[-1])\n",
    "            )\n",
    "        ]\n",
    "        gen_map[img_id] = ordered\n",
    "\n",
    "    # Group paraphrased questions by image_id\n",
    "    paraphr_map = {}\n",
    "    for item in paraphr_data.values():\n",
    "        img_id = item.get(\"image_id\")\n",
    "        if img_id is None:\n",
    "            continue\n",
    "        text = item.get(\"question_generated\", {})\\\n",
    "                   .get(\"question_paraphrased\", \"\")\n",
    "        paraphr_map.setdefault(img_id, []).append(text)\n",
    "\n",
    "    # Merge into final structure with dynamic slots\n",
    "    merged = {}\n",
    "    all_ids = set(qa_map) | set(gen_map) | set(paraphr_map)\n",
    "    for img_id in sorted(all_ids):\n",
    "        key = str(img_id)\n",
    "        merged[key] = {\"image_id\": img_id}\n",
    "\n",
    "        # Original questions/answers\n",
    "        qs = qa_map.get(img_id, {}).get(\"questions\", [])\n",
    "        ans = qa_map.get(img_id, {}).get(\"answers\", [])\n",
    "        merged[key][\"original_question\"] = {\n",
    "            f\"question_{i+1}\": q for i, q in enumerate(qs)\n",
    "        }\n",
    "        merged[key][\"original_answer\"] = {\n",
    "            f\"answer_{i+1}\": a for i, a in enumerate(ans)\n",
    "        }\n",
    "\n",
    "        # Generated questions\n",
    "        gen_list = gen_map.get(img_id, [])\n",
    "        gen_q = {f\"generated_question_{i+1}\": q for i, q in enumerate(gen_list)}\n",
    "\n",
    "        # Paraphrased questions\n",
    "        parap_list = paraphr_map.get(img_id, [])\n",
    "        parap_q = {f\"paraphrased_question_{i+1}\": p for i, p in enumerate(parap_list)}\n",
    "\n",
    "        merged[key][\"qwenvl_generated\"] = {\n",
    "            \"question_paraphrased\": parap_q,\n",
    "            \"question_generated\": gen_q\n",
    "        }\n",
    "\n",
    "    # Write merged JSON\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Saved merged data to {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Merge original Q&A, generated, and paraphrased questions by image_id\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--orig\", required=True,\n",
    "        help=\"Path to original Q&A JSON file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gen\", required=True,\n",
    "        help=\"Path to generated questions JSON file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--paraphr\", required=True,\n",
    "        help=\"Path to paraphrased questions JSON file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\", required=True,\n",
    "        help=\"Path to output merged JSON file\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    merge_json(\n",
    "        orig_path=args.orig,\n",
    "        gen_path=args.gen,\n",
    "        paraphr_path=args.paraphr,\n",
    "        output_path=args.output\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4392084",
   "metadata": {},
   "source": [
    "merge_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "def merge_json_data(original_file, external_file, output_file):\n",
    "    # Load JSON files\n",
    "    with open(original_file, 'r', encoding='utf-8') as f:\n",
    "        original_data = json.load(f)\n",
    "    with open(external_file, 'r', encoding='utf-8') as f:\n",
    "        external_data = json.load(f)\n",
    "\n",
    "    # Map image_id to external entry and extract question lists\n",
    "    ext_by_id = {v['image_id']: v for v in external_data.values()}\n",
    "    paraphrased_lists = {\n",
    "        img_id: list(entry.get('qwenvl_generated', {}).get('question_paraphrased', {}).values())\n",
    "        for img_id, entry in ext_by_id.items()\n",
    "    }\n",
    "    generated_lists = {\n",
    "        img_id: list(entry.get('qwenvl_generated', {}).get('question_generated', {}).values())\n",
    "        for img_id, entry in ext_by_id.items()\n",
    "    }\n",
    "\n",
    "    # Counters to track assignment per annotation\n",
    "    counters = {}\n",
    "\n",
    "    # Iterate annotations in order, assigning questions sequentially\n",
    "    for ann_id, ann_data in original_data.get('annotations', {}).items():\n",
    "        image_id = ann_data.get('image_id')\n",
    "        # Initialize lists and counter\n",
    "        p_list = paraphrased_lists.get(image_id, [])\n",
    "        g_list = generated_lists.get(image_id, [])\n",
    "        idx = counters.get(image_id, 0)\n",
    "\n",
    "        # Pick the next paraphrased and generated question or empty string\n",
    "        pq = p_list[idx] if idx < len(p_list) else \"\"\n",
    "        gq = g_list[idx] if idx < len(g_list) else \"\"\n",
    "\n",
    "        # Attach to annotation\n",
    "        ann_data['qwenvl_generated'] = {\n",
    "            'paraphrased_question': pq,\n",
    "            'generated_question': gq\n",
    "        }\n",
    "\n",
    "        # Increment counter for this image_id\n",
    "        counters[image_id] = idx + 1\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save merged data\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(original_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Merged data saved to {output_file}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Merge JSON data based on image_id')\n",
    "    parser.add_argument('--config', type=str, help='Path to YAML config file')\n",
    "    parser.add_argument('--original', help='Path to original JSON file')\n",
    "    parser.add_argument('--external', help='Path to external JSON file')\n",
    "    parser.add_argument('--output', help='Path to output merged JSON file')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load config if provided\n",
    "    config = {}\n",
    "    if args.config:\n",
    "        with open(args.config, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "\n",
    "    # Merge CLI args and YAML configuraton\n",
    "    merged_config = {}\n",
    "    for key in ['original', 'external', 'output']:\n",
    "        val = getattr(args, key)\n",
    "        if val is not None:\n",
    "            merged_config[key] = val\n",
    "        elif key in config:\n",
    "            merged_config[key] = config[key]\n",
    "\n",
    "    # Validate required params\n",
    "    missing = [k for k in ['original', 'external', 'output'] if k not in merged_config]\n",
    "    if missing:\n",
    "        print(f\"Missing required arguments: {', '.join(missing)}\")\n",
    "        parser.print_help()\n",
    "        return\n",
    "\n",
    "    merge_json_data(\n",
    "        merged_config['original'],\n",
    "        merged_config['external'],\n",
    "        merged_config['output']\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e548acf",
   "metadata": {},
   "source": [
    "Top 10 object extracted by Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import csv, sys, base64, json, os, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Union, Optional\n",
    "import numpy as np\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "FIELDNAMES = [\n",
    "    \"img_id\", \"img_h\", \"img_w\",\n",
    "    \"objects_id\", \"objects_conf\",\n",
    "    \"attrs_id\", \"attrs_conf\",\n",
    "    \"num_boxes\", \"boxes\", \"features\"\n",
    "]\n",
    "\n",
    "# --- 2. Core helpers (unchanged) ---------------------------------------------\n",
    "def load_object_vocab(vocab_file: str | Path) -> Dict[int, str]:\n",
    "    object_names: Dict[int, str] = {}\n",
    "    with open(vocab_file, encoding=\"utf-8\") as f:\n",
    "        for idx, name in enumerate(line.strip() for line in f if line.strip()):\n",
    "            object_names[idx] = name\n",
    "    print(f\"Loaded {len(object_names)} object names from {vocab_file}\")\n",
    "    return object_names\n",
    "\n",
    "\n",
    "def load_obj_tsv(\n",
    "    fname: str | Path,\n",
    "    topk: Optional[int] = None,\n",
    "    verbose: bool = True\n",
    ") -> List[Dict[str, Union[str, int, np.ndarray]]]:\n",
    "    data, start = [], time.time()\n",
    "    if verbose:\n",
    "        print(f\"Loading Faster R-CNN objects from {fname}\")\n",
    "    with open(fname) as f:\n",
    "        reader = csv.DictReader(f, FIELDNAMES, delimiter=\"\\t\")\n",
    "        for i, item in enumerate(reader):\n",
    "            if verbose and i and i % 1000 == 0:\n",
    "                print(f\"  {i} images... ({time.time() - start:.1f}s)\")\n",
    "\n",
    "            # convert scalar fields\n",
    "            for k in (\"img_h\", \"img_w\", \"num_boxes\"):\n",
    "                item[k] = int(item[k])\n",
    "\n",
    "            n = item[\"num_boxes\"]\n",
    "            decode_cfg = [\n",
    "                (\"objects_id\",   (n,),     np.int64),\n",
    "                (\"objects_conf\", (n,),     np.float32),\n",
    "                (\"attrs_id\",     (n,),     np.int64),\n",
    "                (\"attrs_conf\",   (n,),     np.float32),\n",
    "                (\"boxes\",        (n, 4),   np.float32),\n",
    "                (\"features\",     (n, -1),  np.float32),\n",
    "            ]\n",
    "            for key, shape, dtype in decode_cfg:\n",
    "                arr = np.frombuffer(base64.b64decode(item[key]), dtype=dtype).reshape(shape)\n",
    "                arr.setflags(write=False)\n",
    "                item[key] = arr\n",
    "\n",
    "            data.append(item)\n",
    "            if topk and len(data) >= topk:\n",
    "                break\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Loaded {len(data)} images in {time.time() - start:.1f}s\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_top_objects_to_json(\n",
    "    data: List[Dict[str, Union[str, int, np.ndarray]]],\n",
    "    output_file: str | Path,\n",
    "    num_objects: int = 10,\n",
    "    object_mapping: Dict[int, str] | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> Path:\n",
    "    if not data:\n",
    "        raise ValueError(\"`data` is empty—did loading fail?\")\n",
    "\n",
    "    object_mapping = object_mapping or {}\n",
    "    results, start = [], time.time()\n",
    "\n",
    "    for img_idx, item in enumerate(data):\n",
    "        if verbose and img_idx and img_idx % 1000 == 0:\n",
    "            print(f\"  {img_idx} images... ({time.time() - start:.1f}s)\")\n",
    "\n",
    "        n = min(num_objects, item[\"num_boxes\"])\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        top_idx = np.argsort(item[\"objects_conf\"])[-n:][::-1]\n",
    "        entry = {\"img_id\": item[\"img_id\"], \"objects\": []}\n",
    "\n",
    "        for rank, idx in enumerate(top_idx, 1):\n",
    "            obj_id = int(item[\"objects_id\"][idx])\n",
    "            entry[\"objects\"].append({\n",
    "                f\"object_{rank}\": object_mapping.get(obj_id, f\"object_{obj_id}\"),\n",
    "                \"confidence\":   float(item[\"objects_conf\"][idx]),\n",
    "                \"bbox\":         item[\"boxes\"][idx].tolist(),\n",
    "            })\n",
    "\n",
    "        results.append(entry)\n",
    "\n",
    "    output_file = Path(output_file)\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Saved {len(results)} images ⇒ {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "def run_pipeline(\n",
    "    tsv_file: str | Path,\n",
    "    vocab_file: str | Path,\n",
    "    output_json: str | Path,\n",
    "    *,\n",
    "    num_objects: int = 10,\n",
    "    topk: Optional[int] = None,\n",
    "    verbose: bool = True,\n",
    ") -> Path:\n",
    "    \"\"\"One-liner to execute the full extraction inside a notebook cell.\"\"\"\n",
    "    if not Path(tsv_file).exists():\n",
    "        raise FileNotFoundError(tsv_file)\n",
    "    if not Path(vocab_file).exists():\n",
    "        raise FileNotFoundError(vocab_file)\n",
    "\n",
    "    mapping = load_object_vocab(vocab_file)\n",
    "    data = load_obj_tsv(tsv_file, topk=topk, verbose=verbose)\n",
    "    return save_top_objects_to_json(\n",
    "        data, output_json, num_objects=num_objects, object_mapping=mapping, verbose=verbose\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d52e5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1600 object names from /home/hanhpm/task_detection_result_frcnn/VQA_Template/data/1600-400-20/objects_vocab.txt\n",
      "Loading Faster R-CNN objects from /mnt/VLAI_data/detection_features/openvivqa_train_obj36.tsv\n",
      "  1000 images... (3.0s)\n",
      "  2000 images... (6.0s)\n",
      "  3000 images... (9.1s)\n",
      "  4000 images... (12.2s)\n",
      "  5000 images... (15.4s)\n",
      "  6000 images... (18.5s)\n",
      "  7000 images... (21.7s)\n",
      "  8000 images... (24.9s)\n",
      "  9000 images... (28.0s)\n",
      "Loaded 9129 images in 28.4s\n",
      "  1000 images... (0.0s)\n",
      "  2000 images... (0.1s)\n",
      "  3000 images... (0.1s)\n",
      "  4000 images... (0.2s)\n",
      "  5000 images... (0.2s)\n",
      "  6000 images... (0.2s)\n",
      "  7000 images... (0.3s)\n",
      "  8000 images... (0.3s)\n",
      "  9000 images... (0.4s)\n",
      "Saved 9129 images ⇒ /home/duyth/vqa_co_training/vivqa-co-training/data_gen_qwenvl/data/extracted_objects/objects_train.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/duyth/vqa_co_training/vivqa-co-training/data_gen_qwenvl/data/extracted_objects/objects_train.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsv_path   = \"/mnt/VLAI_data/detection_features/openvivqa_train_obj36.tsv\"\n",
    "vocab_path = \"/home/hanhpm/task_detection_result_frcnn/VQA_Template/data/1600-400-20/objects_vocab.txt\"\n",
    "out_path   = \"/home/duyth/vqa_co_training/vivqa-co-training/data_gen_qwenvl/data/extracted_objects/objects_train.json\"\n",
    "\n",
    "run_pipeline(\n",
    "    tsv_file   = tsv_path,\n",
    "    vocab_file = vocab_path,\n",
    "    output_json= out_path,\n",
    "    num_objects= 10,   \n",
    "    topk       = None, \n",
    "    verbose    = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54fdd76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a1d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duyth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
