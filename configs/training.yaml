# Training configuration for supervised-finetuning VLM using Unsloth
# Every parameters in this file is aligned with Unsloth naming convention
trainer_config:
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 1
  learning_rate: 0.0002
  weight_decay: 0.001
  warmup_steps: 5
  # num_train_epochs: 1
  max_steps: 200
  logging_strategy: steps
  logging_steps: 10
  fp16: false
  bf16: true 
  optim: adamw_8bit
  lr_scheduler_type: linear
  seed: 59
  output_dir: training_outputs
  report_to: none
  remove_unused_columns: false
  dataset_text_field: ""
  dataset_kwargs:
    skip_prepare_dataset: true
  dataset_num_proc: 4
  max_seq_length: 512
is_save_model: true
data_collator_choice: "custom"
is_train_resp_only: true
save_name: "sft-vivqa"