# VLM for VQA model configuration
name: "/mnt/dataset1/pretrained_fm/unsloth_Qwen2.5-VL-3B-Instruct"
start_instruct_token: "<|im_start|>user"
start_resp_token: "<|im_start|>assistant"
tokenizer: 
  add_special_tokens: true
quantization:
  is_quantize: true
  quantization_config:
    load_in_4bit: true
    load_in_8bit: false
dtype: bfloat16
instruct_template: "vivqa_instruct_prompt_v1.jinja"
generation_config:
  do_sample: false
  max_new_tokens: 128
  temperature: 0.0001
  top_p: 1.0
  top_k: 50
